{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import Vector_Similarity\n",
    "\n",
    "\n",
    "# TRAIN_SET_PATH = \"20ng-no-stop.txt\"\n",
    "# TRAIN_SET_PATH = \"r52-all-terms.txt\"\n",
    "TRAIN_SET_PATH = \"dataset/r8-no-stop.txt\"\n",
    "\n",
    "GLOVE_6B_50D_PATH = \"dataset/glove.6B.50d.txt\"\n",
    "GLOVE_840B_300D_PATH = \"dataset/glove.6B.300d.txt\"\n",
    "encoding=\"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad para:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad para:  4\n('enwiki:Heavy%20water/Effect%20on%20biological%20systems/Toxicity%20in%20humans', 'Heavy water Effect on biological systems Toxicity in humans', ['heavi', 'water', 'effect', 'biolog', 'system', 'toxic', 'human'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from read_folds import FoldsTraining\n",
    "folds_training = FoldsTraining(nb_folds=2)\n",
    "# relevance_list = [item for sublist in folds_training.true_relevance_stemmed for item in sublist]\n",
    "# relevance_list = folds_training.true_relevance_stemmed[0]\n",
    "queries_list_train = folds_training.queries[0]\n",
    "queries_list_test = folds_training.queries[1]\n",
    "paragraphs_dict = folds_training.paragraphs_dict\n",
    "\n",
    "print(queries_list_train[20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chocol', 'etymolog'] ['unlik', 'true', 'insur', 'credit', 'default', 'swap', 'regul', 'insur', 'provid', 'reserv', 'pay', 'settlement', 'buyer', 'own', 'properti', 'mbss', 'insur', 'ie', 'simpli', 'make', 'bet', 'secur', 'would', 'default', 'synthet', 'referenc', 'anoth', 'cash', 'cdo', 'one', 'fact', 'numer', 'synthet', 'could', 'made', 'refer', 'origin', 'multipli', 'effect', 'referenc', 'secur', 'default', 'mbs', 'cdos', 'tripl', 'rate', 'larg', 'chunk', 'synthet', 'crucial', 'secur', 'success', 'buyerinvestor', 'ignor', 'mortgag', 'secur', 'market', 'trust', 'credit', 'rate', 'agenc', 'rate']\ntotal x = 388 y = 1815\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "y_mapping = {}\n",
    "y_mapping_index = 0\n",
    "i = 0\n",
    "\n",
    "for query in queries_list_train:\n",
    "    X.append(query[2])\n",
    "\n",
    "for par_id, par_text in paragraphs_dict.items():\n",
    "    y.append(par_text)\n",
    "\n",
    "# for query_relevance in relevance_list:\n",
    "#     # if i == 0:\n",
    "#         # print(query_relevance)\n",
    "#     query = query_relevance[0]\n",
    "#     relevance = query_relevance[1]\n",
    "#     for par_id, rel in relevance.items():\n",
    "#         # if i == 0:\n",
    "#             # print(query, par_id, paragraphs_dict[par_id])\n",
    "#             # i += 1\n",
    "#         X.append(query)\n",
    "#         y.append(paragraphs_dict[par_id])\n",
    "\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "print(X[0], y[0])\n",
    "print(\"total x =\", len(X), \"y =\", len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading glove files, this may take a while\n",
    "# we're reading line by line and only saving vectors\n",
    "# that correspond to words from our training set\n",
    "# if you wan't to play around with the vectors and have \n",
    "# enough RAM - remove the 'if' line and load everything\n",
    "\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "glove_small = {}\n",
    "# all_words = set(w for words in X for w in words)\n",
    "# with open(GLOVE_6B_50D_PATH, \"rb\") as infile:\n",
    "#     for line in infile:\n",
    "#         parts = line.split()\n",
    "#         word = parts[0].decode(encoding)\n",
    "#         # if (word in all_words):\n",
    "#         nums=np.array(parts[1:], dtype=np.float32)\n",
    "#         glove_small[stem(word)] = nums\n",
    "\n",
    "            \n",
    "glove_big = {}\n",
    "with open(GLOVE_840B_300D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        nums=np.array(parts[1:], dtype=np.float32)\n",
    "        glove_small[stem(word)] = nums\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec) > 0:\n",
    "            self.dim = len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                     for w in words if w in self.word2vec] or\n",
    "                    [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfEmbeddingVectorizer(glove_small).fit(np.append(X, y), y=None)\n",
    "# tfidf_y = TfidfEmbeddingVectorizer(glove_small).fit(y, y=None)\n",
    "# x_vec = tfidf.transform(X)\n",
    "# y_vec = tfidf.transform(y)\n",
    "\n",
    "x_vec_X = tfidf.transform(X)\n",
    "y_vec_X = tfidf.transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\workspace\\python\\complex-retrieval\\Vector_Similarity.py:5: RuntimeWarning: invalid value encountered in double_scalars\n  result = InnerProduct(vec1, vec2) / (VectorSize(vec1) * VectorSize(vec2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0.299\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "topn = 20\n",
    "\n",
    "# query = queries_list_test[10]\n",
    "final_scores_tsss = {}\n",
    "final_scores_cosine = {}\n",
    "stop = 0\n",
    "progress = 0\n",
    "for query in queries_list_test:\n",
    "    q = query[2]\n",
    "    # print(queries_list_test[10])\n",
    "    # print(q)\n",
    "\n",
    "    scores_tsss = {}\n",
    "    scores_cosine = {}\n",
    "    q_vec = tfidf.transform([q])[0]\n",
    "    for par_id, par_text in paragraphs_dict.items():\n",
    "        par_text_vec = tfidf.transform([par_text])[0]\n",
    "        scores_tsss[par_id] = Vector_Similarity.TS_SS(q_vec, par_text_vec)\n",
    "        scores_cosine[par_id] = Vector_Similarity.Cosine(q_vec, par_text_vec)\n",
    "\n",
    "    sorted_scores_tsss = sorted(scores_tsss.items(), key=operator.itemgetter(1), reverse=True)[:topn]\n",
    "    sorted_scores_cosine = sorted(scores_cosine.items(), key=operator.itemgetter(1), reverse=True)[:topn]\n",
    "\n",
    "    for tsss in sorted_scores_tsss:\n",
    "        if final_scores_tsss.get(query[0]) is None:\n",
    "            final_scores_tsss[query[0]] = []\n",
    "        final_scores_tsss[query[0]].append([tsss[0], tsss[1]])\n",
    "\n",
    "    for cosine in sorted_scores_tsss:\n",
    "        if final_scores_cosine.get(query[0]) is None:\n",
    "            final_scores_cosine[query[0]] = []\n",
    "        final_scores_cosine[query[0]].append([cosine[0], cosine[1]])\n",
    "\n",
    "    progress += 1\n",
    "    print(\"progress:\", \"%.3f\" % round(progress/len(queries_list_test), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4280\n"
     ]
    }
   ],
   "source": [
    "from trec_car.format_runs import *\n",
    "stop = 0\n",
    "output_entries_tsss = []\n",
    "for query_id, paragraphs in final_scores_tsss.items():     \n",
    "    rank = 1\n",
    "    for paragraph_score in paragraphs:        \n",
    "        entry = RankingEntry(query_id, paragraph_score[0], rank, paragraph_score[1])\n",
    "        output_entries_tsss.append(entry)\n",
    "        rank += 1\n",
    "        \n",
    "print(len(output_entries_tsss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_entries_cosine = []\n",
    "for query_id, paragraphs in final_scores_cosine.items():     \n",
    "    rank = 1\n",
    "    for paragraph_score in paragraphs:        \n",
    "        entry = RankingEntry(query_id, paragraph_score[0], rank, paragraph_score[1])\n",
    "        output_entries_cosine.append(entry)\n",
    "        rank += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scores_to_file(output_entries, filename=\"test.out\"):\n",
    "    with open(filename, mode='w', encoding='UTF-8') as f:\n",
    "        writer = f\n",
    "        temp_list = []\n",
    "        for entry in output_entries:\n",
    "            temp_list.append(entry)\n",
    "        format_run(writer, temp_list, exp_name='test')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enwiki:Photosynthesis/Overview 8d7f0d3e442177be4ac18b4cdd612440436763b1 12 3446.3602986864043\n"
     ]
    }
   ],
   "source": [
    "o = output_entries_tsss[11]\n",
    "print(o.query_id, o.paragraph_id, o.rank, o.score)\n",
    "save_scores_to_file(output_entries_tsss, filename=\"word2vec_tsss_stemglove_top20.out\")\n",
    "save_scores_to_file(output_entries_cosine, filename=\"word2vec_cosine_stemglove_top20.out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
